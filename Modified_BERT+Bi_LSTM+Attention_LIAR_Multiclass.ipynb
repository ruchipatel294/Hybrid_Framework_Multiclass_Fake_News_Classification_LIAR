{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "74eb5a95-20f1-4145-8d4c-254676eb0264",
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q scikit-learn seaborn contractions nltk gensim\n",
        "\n",
        "import os, re, time, random, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import contractions\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, confusion_matrix, classification_report)\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import nltk\n",
        "nltk.download('wordnet', quiet=True)\n",
        "from nltk.corpus import wordnet\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "trusted": true,
        "id": "74eb5a95-20f1-4145-8d4c-254676eb0264"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5719b677-3b54-489a-82c1-9fe9bf7670b3",
      "cell_type": "code",
      "source": [
        "\n",
        "MODEL_NAME = \"bert-base-uncased\"   # keep architecture same\n",
        "MAX_LEN = 256                      # reduce to 128 if OOM\n",
        "BATCH_SIZE = 8                     # adjust if OOM\n",
        "EPOCHS = 10\n",
        "LR = 2e-5\n",
        "N_SPLITS = 5\n",
        "SEED = 42\n",
        "GRAD_ACCUM = 1      # set >1 for gradient accumulation\n",
        "WEIGHT_DECAY = 0.01\n",
        "DROPOUT = 0.4\n",
        "EARLYSTOP_PATIENCE = 3\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "5719b677-3b54-489a-82c1-9fe9bf7670b3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5e367089-551c-4003-9cd1-3cb0914bdc1d",
      "cell_type": "code",
      "source": [
        "\n",
        "input_root = '/kaggle/input'\n",
        "csvs = []\n",
        "for root, dirs, files in os.walk(input_root):\n",
        "    for f in files:\n",
        "        if f.lower().endswith('.csv'):\n",
        "            csvs.append(os.path.join(root, f))\n",
        "if len(csvs) == 0:\n",
        "    raise FileNotFoundError(\"No CSV found in /kaggle/input. Upload your dataset.\")\n",
        "csv_path = csvs[0]\n",
        "print(\"Using CSV:\", csv_path)\n",
        "raw_df = pd.read_csv(csv_path)\n",
        "print(\"Raw shape:\", raw_df.shape)\n",
        "display(raw_df.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "5e367089-551c-4003-9cd1-3cb0914bdc1d"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "82a45450-e588-47ce-8603-f3d741fb2be7",
      "cell_type": "code",
      "source": [
        "\n",
        "label_map = {\n",
        "    'pants-fire': 0, 'pants on fire': 0, 'pants on fire.': 0,\n",
        "    'false': 1,\n",
        "    'barely-true': 2, 'barely true': 2,\n",
        "    'half-true': 3, 'half true': 3,\n",
        "    'mostly-true': 4, 'mostly true': 4,\n",
        "    'true': 5\n",
        "}\n",
        "\n",
        "\n",
        "def find_col(df, names):\n",
        "    for c in df.columns:\n",
        "        if c.lower() in names:\n",
        "            return c\n",
        "    return None\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "82a45450-e588-47ce-8603-f3d741fb2be7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "23a46796-82e1-48d8-9913-3218fdfa997b",
      "cell_type": "code",
      "source": [
        "stmt_col = find_col(raw_df, ['statement','statements','claim','text','statement_text'])\n",
        "label_col = find_col(raw_df, ['label','labels','label_mapped','truth','veracity','class'])\n",
        "speaker_col = find_col(raw_df, ['speaker','speaker_name','speakers','speaker_id'])\n",
        "party_col = find_col(raw_df, ['party','political_party'])\n",
        "subject_col = find_col(raw_df, ['subject','topic'])\n",
        "context_col = find_col(raw_df, ['context','source','venue'])\n",
        "\n",
        "if stmt_col is None or label_col is None:\n",
        "    raise ValueError(\"Couldn't auto-detect 'statement' or 'label' columns. Rename CSV columns accordingly.\")\n",
        "print(\"Detected columns -> statement:\", stmt_col, \", label:\", label_col)\n",
        "meta_cols = {}\n",
        "if speaker_col: meta_cols['speaker'] = speaker_col\n",
        "if party_col: meta_cols['party'] = party_col\n",
        "if subject_col: meta_cols['subject'] = subject_col\n",
        "if context_col: meta_cols['context'] = context_col\n",
        "print(\"Detected metadata columns:\", meta_cols)\n",
        "\n",
        "raw_df['label_raw'] = raw_df[label_col].astype(str).str.lower().str.strip()\n",
        "raw_df['label_mapped'] = raw_df['label_raw'].map(label_map)\n",
        "if raw_df['label_mapped'].isna().any():\n",
        "    print(\"Unmapped labels examples (first 20):\", raw_df[raw_df['label_mapped'].isna()][label_col].unique()[:20])"
      ],
      "metadata": {
        "trusted": true,
        "id": "23a46796-82e1-48d8-9913-3218fdfa997b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "47bf0c82-dd34-40f1-86da-64146d732eb3",
      "cell_type": "code",
      "source": [
        "\n",
        "keep_cols = [stmt_col, 'label_mapped']\n",
        "for v in meta_cols.values():\n",
        "    keep_cols.append(v)\n",
        "DF = raw_df[keep_cols].dropna(subset=[stmt_col,'label_mapped']).rename(columns={stmt_col:'statement','label_mapped':'label'}).reset_index(drop=True)\n",
        "DF['label'] = DF['label'].astype(int)\n",
        "print(\"Prepared DF shape:\", DF.shape)\n",
        "print(DF['label'].value_counts())"
      ],
      "metadata": {
        "trusted": true,
        "id": "47bf0c82-dd34-40f1-86da-64146d732eb3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_person_names(text):\n",
        "    toks = text.split()\n",
        "    i = 0; out=[]\n",
        "    while i < len(toks):\n",
        "        if toks[i][0:1].isupper() and toks[i].isalpha():\n",
        "            j=i+1; seq=[toks[i]]\n",
        "            while j<len(toks) and toks[j][0:1].isupper() and toks[j].isalpha():\n",
        "                seq.append(toks[j]); j+=1\n",
        "            if len(seq)>=1:\n",
        "                out.append(\"<person>\"); i=j; continue\n",
        "        out.append(toks[i]); i+=1\n",
        "    return \" \".join(out)\n",
        "\n",
        "def clean_text(text):\n",
        "    if not isinstance(text,str): return \"\"\n",
        "    text = text.strip().replace('\\n',' ').replace('\\r',' ')\n",
        "    text = contractions.fix(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)\n",
        "    text = re.sub(r'\\[.*?\\]|\\(.*?\\)|\\<.*?\\>', ' ', text)\n",
        "    text = re.sub(r\"[^a-z0-9\\s'\\.,!?;-]\", ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
        "    toks = text.split()\n",
        "    for i in range(len(toks)-1):\n",
        "        if toks[i] in ('not','no','never'):\n",
        "            toks[i] = toks[i] + \"_\" + toks[i+1]\n",
        "            toks[i+1] = ''\n",
        "    text = \" \".join([t for t in toks if t])\n",
        "    text = mask_person_names(text)\n",
        "    return text\n",
        "\n",
        "DF['clean_statement'] = DF['statement'].astype(str).map(clean_text)\n",
        "DF = DF[DF['clean_statement'].str.len() > 5].reset_index(drop=True)\n",
        "print(\"After cleaning:\", DF.shape)"
      ],
      "metadata": {
        "id": "fndLbNiUKqz8"
      },
      "id": "fndLbNiUKqz8",
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "74502823-8a15-44e4-8185-36866d24b641",
      "cell_type": "code",
      "source": [
        "# ============ Topic modelling (LDA) - add topic_id metadata ============\n",
        "n_topics = 10\n",
        "vectorizer = CountVectorizer(max_df=0.9, min_df=5, stop_words='english')\n",
        "X_counts = vectorizer.fit_transform(DF['clean_statement'].values)\n",
        "lda = LatentDirichletAllocation(n_components=n_topics, random_state=SEED, n_jobs=-1)\n",
        "topic_dist = lda.fit_transform(X_counts)\n",
        "DF['topic_id'] = topic_dist.argmax(axis=1)\n",
        "print(\"Topic distribution:\", DF['topic_id'].value_counts().to_dict())"
      ],
      "metadata": {
        "trusted": true,
        "id": "74502823-8a15-44e4-8185-36866d24b641"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "fcbb2bd9-eece-4029-b00e-738cc693356a",
      "cell_type": "code",
      "source": [
        "# ============ Inject metadata and speaker history into model input ============\n",
        "def make_model_input(row):\n",
        "    parts = [row['clean_statement']]\n",
        "    if 'speaker' in DF.columns and pd.notna(row.get('speaker')):\n",
        "        parts.append(f\"speaker={row.get('speaker')}\")\n",
        "    if 'party' in DF.columns and pd.notna(row.get('party')):\n",
        "        parts.append(f\"party={row.get('party')}\")\n",
        "    if 'subject' in DF.columns and pd.notna(row.get('subject')):\n",
        "        parts.append(f\"subject={row.get('subject')}\")\n",
        "    if 'context' in DF.columns and pd.notna(row.get('context')):\n",
        "        parts.append(f\"context={row.get('context')}\")\n",
        "    parts.append(f\"topic={int(row['topic_id'])}\")\n",
        "    return \" [SEP] \".join(parts)\n",
        "\n",
        "DF['model_input'] = DF.apply(make_model_input, axis=1)\n",
        "\n",
        "# speaker history: true rate per speaker if available\n",
        "if 'speaker' in DF.columns:\n",
        "    speaker_true_rate = DF.groupby('speaker')['label'].apply(lambda arr: np.mean(np.array(arr)==5)).to_dict()\n",
        "    DF['speaker_true_rate'] = DF['speaker'].map(speaker_true_rate).fillna(0.0)\n",
        "    DF['model_input'] = DF.apply(lambda r: r['model_input'] + f\" [SEP] speaker_true_rate={r['speaker_true_rate']:.3f}\", axis=1)\n",
        "else:\n",
        "    DF['speaker_true_rate'] = 0.0\n",
        "\n",
        "print(\"Sample model_input:\")\n",
        "display(DF[['model_input','label']].head())\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "fcbb2bd9-eece-4029-b00e-738cc693356a"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a9590f3d-5f15-4815-9a73-0e941a79398c",
      "cell_type": "code",
      "source": [
        "# ============ Tokenizer & Dataset class (with augmentation) ============\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "def synonym_replace(text, p=0.05):\n",
        "    toks = text.split()\n",
        "    out=[]\n",
        "    for t in toks:\n",
        "        if random.random() < p:\n",
        "            syns = wordnet.synsets(t)\n",
        "            if syns:\n",
        "                lemmas = [l.name().replace('_',' ') for s in syns for l in s.lemmas()]\n",
        "                lemmas = [w for w in lemmas if w.lower()!=t.lower() and w.isalpha()]\n",
        "                if lemmas:\n",
        "                    out.append(random.choice(lemmas)); continue\n",
        "        out.append(t)\n",
        "    return \" \".join(out)"
      ],
      "metadata": {
        "trusted": true,
        "id": "a9590f3d-5f15-4815-9a73-0e941a79398c"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e10cbfe1-d7a2-4d46-8174-08c29ea43fb5",
      "cell_type": "code",
      "source": [
        "class LIARDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len, augment=False):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.augment = augment\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        if self.augment:\n",
        "            if random.random() < 0.1:\n",
        "                text = synonym_replace(text, p=0.05)\n",
        "            if random.random() < 0.05:\n",
        "                toks = text.split()\n",
        "                if len(toks)>3:\n",
        "                    pos = random.randint(0,len(toks)-1)\n",
        "                    toks[pos] = '[MASK]'; text = ' '.join(toks)\n",
        "        enc = tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
        "        return {'ids': enc['input_ids'].squeeze(0), 'mask': enc['attention_mask'].squeeze(0), 'label': torch.tensor(int(self.labels[idx]), dtype=torch.long)}\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "e10cbfe1-d7a2-4d46-8174-08c29ea43fb5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9daf054c-df6a-45f7-a828-fe8934f6e98d",
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ============ Model: BERT + BiLSTM + Attention ============\n",
        "class BertBiLSTMAttn(nn.Module):\n",
        "    def __init__(self, model_name, lstm_hidden=128, num_classes=6, dropout=DROPOUT, freeze_bert=False):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        if freeze_bert:\n",
        "            for p in self.bert.parameters(): p.requires_grad=False\n",
        "        self.lstm = nn.LSTM(input_size=self.bert.config.hidden_size, hidden_size=lstm_hidden, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.attn = nn.Linear(lstm_hidden*2, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(lstm_hidden*2, num_classes)\n",
        "    def forward(self, ids, mask):\n",
        "        bert_out = self.bert(ids, attention_mask=mask)\n",
        "        seq_output = bert_out.last_hidden_state\n",
        "        lstm_out, _ = self.lstm(seq_output)\n",
        "        attn_weights = torch.softmax(self.attn(lstm_out), dim=1)\n",
        "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
        "        context = self.dropout(context)\n",
        "        out = self.classifier(context)\n",
        "        return out"
      ],
      "metadata": {
        "trusted": true,
        "id": "9daf054c-df6a-45f7-a828-fe8934f6e98d"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "096e7ed1-f334-43e0-8163-db1defd997af",
      "cell_type": "code",
      "source": [
        "# ============ Training / Eval / EarlyStopping utilities ============\n",
        "def train_epoch(model, loader, optimizer, scheduler, criterion, grad_accum_steps=1):\n",
        "    model.train()\n",
        "    total_loss=0.0; total_correct=0; total_samples=0\n",
        "    optimizer.zero_grad()\n",
        "    for step, batch in enumerate(loader, 1):\n",
        "        ids = batch['ids'].to(device); mask = batch['mask'].to(device); labels = batch['label'].to(device)\n",
        "        outputs = model(ids, mask)\n",
        "        loss = criterion(outputs, labels) / grad_accum_steps\n",
        "        loss.backward()\n",
        "        if step % grad_accum_steps == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            if scheduler is not None: scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "        total_loss += loss.item() * grad_accum_steps\n",
        "        preds = outputs.argmax(1)\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    avg_acc = total_correct / total_samples if total_samples>0 else 0\n",
        "    return avg_loss, avg_acc"
      ],
      "metadata": {
        "trusted": true,
        "id": "096e7ed1-f334-43e0-8163-db1defd997af"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "99a3b1a8-5972-447e-a8ed-4c78e409b6fe",
      "cell_type": "code",
      "source": [
        "def eval_model(model, loader, criterion=None):\n",
        "    model.eval()\n",
        "    all_preds=[]; all_labels=[]\n",
        "    total_loss=0.0; total_samples=0\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            ids = batch['ids'].to(device); mask = batch['mask'].to(device); labels = batch['label'].to(device)\n",
        "            outputs = model(ids, mask)\n",
        "            preds = outputs.argmax(1)\n",
        "            all_preds.extend(preds.cpu().numpy()); all_labels.extend(labels.cpu().numpy())\n",
        "            if criterion is not None:\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_loss += loss.item(); total_samples += labels.size(0)\n",
        "    avg_loss = (total_loss / len(loader)) if (criterion is not None and len(loader)>0) else None\n",
        "    acc = accuracy_score(all_labels, all_preds) if len(all_labels)>0 else 0\n",
        "    return avg_loss, acc, np.array(all_labels), np.array(all_preds)"
      ],
      "metadata": {
        "trusted": true,
        "id": "99a3b1a8-5972-447e-a8ed-4c78e409b6fe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c88f5b1d-a227-4b73-8e23-9f57e6ae605a",
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, verbose=False, delta=0.0, path='checkpoint.pt'):\n",
        "        self.patience=patience; self.verbose=verbose; self.delta=delta; self.path=path\n",
        "        self.best_score=None; self.counter=0; self.early_stop=False\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "        if self.best_score is None:\n",
        "            self.best_score=score; self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose: print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience: self.early_stop = True\n",
        "        else:\n",
        "            self.best_score=score; self.save_checkpoint(val_loss, model); self.counter=0\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        torch.save(model.state_dict(), self.path)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "c88f5b1d-a227-4b73-8e23-9f57e6ae605a"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2975644b-8ca7-4d82-9418-be35c57a09ae",
      "cell_type": "code",
      "source": [
        "# ============ Prepare class weights & global criterion ============\n",
        "classes = np.unique(DF['label'])\n",
        "class_weights_np = compute_class_weight(class_weight='balanced', classes=classes, y=DF['label'].values)\n",
        "class_weights = torch.tensor(class_weights_np, dtype=torch.float).to(device)\n",
        "print(\"Class weights:\", class_weights_np)\n",
        "criterion_global = nn.CrossEntropyLoss(weight=class_weights)"
      ],
      "metadata": {
        "trusted": true,
        "id": "2975644b-8ca7-4d82-9418-be35c57a09ae"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "da1767fd-bbd2-4f25-913b-41d045f8a8fa",
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ============ K-Fold training (with metrics) ============\n",
        "all_train_acc=[]; all_val_acc=[]; all_train_loss=[]; all_val_loss=[]\n",
        "fold_accuracies=[]; class_names = [\"pants-fire\",\"false\",\"barely-true\",\"half-true\",\"mostly-true\",\"true\"]\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(DF, DF['label']), 1):\n",
        "    print(f\"\\n\\n===== FOLD {fold}/{N_SPLITS} =====\")\n",
        "    train_df = DF.iloc[train_idx].reset_index(drop=True); val_df = DF.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "    train_data = LIARDataset(train_df['model_input'].values, train_df['label'].values, tokenizer, MAX_LEN, augment=True)\n",
        "    val_data = LIARDataset(val_df['model_input'].values, val_df['label'].values, tokenizer, MAX_LEN, augment=False)\n",
        "\n",
        "    train_labels = train_df['label'].values\n",
        "    sample_weights = np.array([class_weights_np[label] for label in train_labels], dtype=np.double)\n",
        "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, sampler=sampler, drop_last=False)\n",
        "    val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    model = BertBiLSTMAttn(MODEL_NAME, lstm_hidden=128, num_classes=6, dropout=DROPOUT, freeze_bert=False).to(device)\n",
        "\n",
        "    # freeze lower bert layers initially (layers 0..7)\n",
        "    freeze_bert_layers(model, freeze_until=8)\n",
        "    print(\"Initially frozen BERT layers 0..7 and embeddings\")\n",
        "\n",
        "    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    total_steps = max(1, len(train_loader) * EPOCHS // GRAD_ACCUM)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1*total_steps), num_training_steps=total_steps)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    early_stopper = EarlyStopping(patience=EARLYSTOP_PATIENCE, verbose=True, path=f'/kaggle/working/best_fold{fold}.pt')\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "        # schedule: epoch 1-3 freeze lower 0..7; epoch4 freeze_until=10; epoch7 unfreeze all\n",
        "        if epoch == 4:\n",
        "            freeze_bert_layers(model, freeze_until=10)\n",
        "            print(\"At epoch 4: now frozen layers 0..9 (so training layers 10..11)\")\n",
        "            optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "            total_steps = max(1, len(train_loader) * (EPOCHS - epoch + 1) // GRAD_ACCUM)\n",
        "            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1*total_steps), num_training_steps=total_steps)\n",
        "        if epoch == 7:\n",
        "            unfreeze_all_bert(model)\n",
        "            print(\"At epoch 7: fully unfreezed BERT\")\n",
        "            optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR/2, weight_decay=WEIGHT_DECAY)\n",
        "            total_steps = max(1, len(train_loader) * (EPOCHS - epoch + 1) // GRAD_ACCUM)\n",
        "            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1*total_steps), num_training_steps=total_steps)\n",
        "\n",
        "        print(f\"\\n--- Fold {fold} | Epoch {epoch}/{EPOCHS} ---\")\n",
        "        t0 = time.time()\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, criterion, grad_accum_steps=GRAD_ACCUM)\n",
        "        val_loss, val_acc, y_true, y_pred = eval_model(model, val_loader, criterion)\n",
        "\n",
        "        all_train_acc.append(train_acc); all_val_acc.append(val_acc)\n",
        "        all_train_loss.append(train_loss); all_val_loss.append(val_loss if val_loss is not None else (1-val_acc))\n",
        "\n",
        "        print(f\"Train Loss {train_loss:.4f} | Train Acc {train_acc:.4f}\")\n",
        "        print(f\"Val   Loss {val_loss:.4f} | Val   Acc {val_acc:.4f} | Time {time.time()-t0:.1f}s\")\n",
        "\n",
        "        # === Overall metrics (already val_acc given) ===\n",
        "        overall_precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "        overall_recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "        overall_f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "        overall_accuracy = accuracy_score(y_true, y_pred)\n",
        "        print(\"\\nOverall metrics:\")\n",
        "        print(f\"Accuracy: {overall_accuracy:.4f}  Precision: {overall_precision:.4f}  Recall: {overall_recall:.4f}  F1: {overall_f1:.4f}\")\n",
        "\n",
        "        # === Class-wise metrics ===\n",
        "        print(\"\\nClass-wise metrics (precision / recall / f1 / support):\")\n",
        "        print(classification_report(y_true, y_pred, target_names=class_names, digits=4, zero_division=0))\n",
        "\n",
        "        # === Confusion matrix ===\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        plt.figure(figsize=(7,5))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.title(f\"Fold {fold} Confusion Matrix - Epoch {epoch}\")\n",
        "        plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\"); plt.show()\n",
        "\n",
        "        # save best model by val_acc\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), f'/kaggle/working/bert_bilstm_attn_fold{fold}.pt')\n",
        "            print(f\"Saved best model for fold {fold} (val_acc={best_val_acc:.4f})\")\n",
        "\n",
        "        early_stopper(val_loss if val_loss is not None else (1 - val_acc), model)\n",
        "        if early_stopper.early_stop:\n",
        "            print(\"Early stopping triggered for this fold.\")\n",
        "            break\n",
        "\n",
        "    fold_accuracies.append(best_val_acc)\n",
        "    print(f\"Best val acc fold {fold}: {best_val_acc:.4f}\")\n",
        "\n",
        "print(\"\\nCross-validation fold accuracies:\", fold_accuracies)\n",
        "print(\"Mean accuracy:\", np.mean(fold_accuracies))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "da1767fd-bbd2-4f25-913b-41d045f8a8fa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1488db00-643e-4c48-9092-08895625533a",
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(all_train_acc, label='Train Acc')\n",
        "plt.plot(all_val_acc, label='Val Acc')\n",
        "plt.xlabel(\"Epoch steps\"); plt.ylabel(\"Accuracy\"); plt.legend(); plt.title(\"Train & Val Accuracy\"); plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(all_train_loss, label='Train Loss')\n",
        "plt.plot(all_val_loss, label='Val Loss')\n",
        "plt.xlabel(\"Epoch steps\"); plt.ylabel(\"Loss\"); plt.legend(); plt.title(\"Train & Val Loss\"); plt.show()\n",
        "\n",
        "best_path = '/kaggle/working/bert_bilstm_attn_fold1.pt'\n",
        "if os.path.exists(best_path):\n",
        "    print(\"Loading saved best model from:\", best_path)\n",
        "    model = BertBiLSTMAttn(MODEL_NAME, lstm_hidden=128, num_classes=6, dropout=DROPOUT, freeze_bert=False).to(device)\n",
        "    model.load_state_dict(torch.load(best_path))\n",
        "    model.eval()\n",
        "else:\n",
        "    print(\"No saved model for fold1 found.\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "1488db00-643e-4c48-9092-08895625533a"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}